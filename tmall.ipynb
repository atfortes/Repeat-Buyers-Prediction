{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Big Data Intelligence Project - TMALL Repeat Buyers**\n",
    "### **Armando Fortes, David Pissarra, Gabriele Oliaro**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBClassifier, DMatrix, train\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR_1 = '../data_format1/'\n",
    "DATA_DIR_2 = '../data_format2/'\n",
    "TRAIN_PATH = DATA_DIR_1 + 'train_format1.csv'\n",
    "TEST_PATH = DATA_DIR_1 + 'test_format1.csv'\n",
    "USER_INFO_PATH = DATA_DIR_1 + 'user_info_format1.csv'\n",
    "USER_LOG_PATH = DATA_DIR_1 + 'user_log_format1.csv'\n",
    "DOUBLE11_DAY = 184"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_SET_SIZE = 0.2\n",
    "PCA_COMPONENTS = 5\n",
    "RANDOM_SEED = 42\n",
    "EPSILON = 1e-10\n",
    "SPLITS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Pre-Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(TRAIN_PATH)\n",
    "df_user_info = pd.read_csv(USER_INFO_PATH)\n",
    "df_user_log = pd.read_csv(USER_LOG_PATH)\n",
    "df_test = pd.read_csv(TEST_PATH)\n",
    "df_test.drop('prob', axis=1, inplace=True)\n",
    "\n",
    "df_train['kind'] = 'train'\n",
    "df_test['kind'] = 'test'\n",
    "df = df_train.append(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize memory usage and Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{round(df_user_log.memory_usage().sum() / 2**30, 2)} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_log['user_id'] = df_user_log['user_id'].astype(np.int32)\n",
    "df_user_log['item_id'] = df_user_log['item_id'].astype(np.int32)\n",
    "df_user_log['cat_id'] = df_user_log['cat_id'].astype(np.int16)\n",
    "df_user_log['seller_id'] = df_user_log['seller_id'].astype(np.int16)\n",
    "df_user_log.rename(columns={'seller_id' : 'merchant_id'}, inplace=True)\n",
    "df_user_log['brand_id'].fillna(0, inplace=True)\n",
    "df_user_log['brand_id'] = df_user_log['brand_id'].astype(np.int16)\n",
    "df_user_log['time_stamp'] = (pd.to_datetime(df_user_log['time_stamp'], format='%m%d') - pd.to_datetime(df_user_log['time_stamp'].min(), format='%m%d')).dt.days\n",
    "df_user_log['time_stamp'] = df_user_log['time_stamp'].astype(np.int16)\n",
    "df_user_log['action_type'] = df_user_log['action_type'].astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{round(df_user_log.memory_usage().sum() / 2**30, 2)} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_info['age_range'].fillna(0, inplace=True)\n",
    "df_user_info['gender'].fillna(2, inplace=True)\n",
    "df_user_info['age_range'] = df_user_info['age_range'].astype(np.int8)\n",
    "df_user_info['gender'] = df_user_info['gender'].astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = df_user_log.groupby('user_id')\n",
    "merchants = df_user_log.groupby('merchant_id')\n",
    "users_merchants = df_user_log.groupby(['user_id', 'merchant_id'])\n",
    "\n",
    "double11 = (df_user_log[df_user_log['time_stamp'] == DOUBLE11_DAY]).reset_index(drop=True)\n",
    "double11_users = double11.groupby('user_id')\n",
    "double11_merchants = double11.groupby('merchant_id')\n",
    "double11_users_merchants = double11.groupby(['user_id', 'merchant_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General counting and ratio features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform age categorical features into different binary features\n",
    "to_merge = pd.get_dummies(df_user_info, prefix='age', columns=['age_range'])\n",
    "df = df.merge(to_merge, on='user_id', how='left')\n",
    "\n",
    "# count total number of unique values from each feature for a given user \n",
    "to_merge = users.nunique().reset_index().rename(columns={\n",
    "    'item_id': 'items_user', \n",
    "    'cat_id': 'categories_user',\n",
    "    'merchant_id': 'merchants_user',\n",
    "    'brand_id': 'brands_user',\n",
    "    'time_stamp': 'dates_user',\n",
    "    'action_type': 'action_types_user'\n",
    "    })\n",
    "df = df.merge(to_merge, on='user_id', how='left')\n",
    "\n",
    "# count total number of unique values from each feature for a given merchant \n",
    "to_merge = merchants.nunique().reset_index().rename(columns={\n",
    "    'item_id': 'items_merchant', \n",
    "    'cat_id': 'categories_merchant',\n",
    "    'user_id': 'userss_merchant',\n",
    "    'brand_id': 'brands_merchant',\n",
    "    'time_stamp': 'dates_merchant',\n",
    "    'action_type': 'action_types_merchant'\n",
    "    })\n",
    "df = df.merge(to_merge, on='merchant_id', how='left')\n",
    "\n",
    "# count total number of unique values from each feature for a given user and merchant\n",
    "to_merge = users_merchants.nunique().reset_index().rename(columns={\n",
    "    'item_id': 'items_user_merchant', \n",
    "    'cat_id': 'categories_user_merchant',\n",
    "    'brand_id': 'brands_user_merchant',\n",
    "    'time_stamp': 'dates_user_merchant',\n",
    "    'action_type': 'action_types_user_merchant'\n",
    "    })\n",
    "df = df.merge(to_merge, on=['user_id', 'merchant_id'], how='left')\n",
    "\n",
    "# count total actions by type for a given user\n",
    "to_merge = users['action_type'].value_counts().unstack(fill_value=0).rename(columns={\n",
    "    0: 'clicks_user',\n",
    "    1: 'carts_user',\n",
    "    2: 'purchases_user',\n",
    "    3: 'favourites_user'\n",
    "    })\n",
    "df = df.merge(to_merge, on='user_id', how='left')\n",
    "\n",
    "# count total actions by type for a given merchant\n",
    "to_merge = merchants['action_type'].value_counts().unstack(fill_value=0).rename(columns={\n",
    "    0: 'clicks_merchant', \n",
    "    1: 'carts_merchant',\n",
    "    2: 'purchases_merchant',\n",
    "    3: 'favourites_merchant'\n",
    "    })\n",
    "df = df.merge(to_merge, on='merchant_id', how='left')\n",
    "\n",
    "# count total actions by type for a given pair (user, merchant)\n",
    "to_merge = users_merchants['action_type'].value_counts().unstack(fill_value=0).rename(columns={\n",
    "    0: 'clicks_user_merchant',\n",
    "    1: 'carts_user_merchant',\n",
    "    2: 'purchases_user_merchant',\n",
    "    3: 'favourites_user_merchant'\n",
    "    })\n",
    "df = df.merge(to_merge, on=['user_id', 'merchant_id'], how='left')\n",
    "\n",
    "# ratio of actions in each merchant (user perspective)\n",
    "df['clicks_in_merchant_ratio_perspective'] = df['clicks_user_merchant'] / (df['clicks_user'] + EPSILON)\n",
    "df['carts_in_merchant_ratio_perspective'] = df['carts_user_merchant'] / (df['carts_user'] + EPSILON)\n",
    "df['purchases_in_merchant_ratio_perspective'] = df['purchases_user_merchant'] / (df['purchases_user'] + EPSILON)\n",
    "df['favourites_in_merchant_ratio_perspective'] = df['favourites_user_merchant'] / (df['favourites_user'] + EPSILON)\n",
    "\n",
    "# ratio of actions in each merchant (merchant perspective)\n",
    "df['clicks_by_user_ratio_perspective'] = df['clicks_user_merchant'] / (df['clicks_merchant'] + EPSILON)\n",
    "df['carts_by_user_ratio_perspective'] = df['carts_user_merchant'] / (df['carts_merchant'] + EPSILON)\n",
    "df['purchases_by_user_ratio_perspective'] = df['purchases_user_merchant'] / (df['purchases_merchant'] + EPSILON)\n",
    "df['favourites_by_user_ratio_perspective'] = df['favourites_user_merchant'] / (df['favourites_merchant'] + EPSILON)\n",
    "\n",
    "# ratio of each action type for a given user\n",
    "df['clicks_user_ratio'] = df['clicks_user'] / (df['clicks_user'] + df['carts_user'] + df['purchases_user'] + df['favourites_user'] + EPSILON)\n",
    "df['carts_user_ratio'] = df['carts_user'] / (df['clicks_user'] + df['carts_user'] + df['purchases_user'] + df['favourites_user'] + EPSILON)\n",
    "df['purchases_user_ratio'] = df['purchases_user'] / (df['clicks_user'] + df['carts_user'] + df['purchases_user'] + df['favourites_user'] + EPSILON)\n",
    "df['favourites_user_ratio'] = df['favourites_user'] / (df['clicks_user'] + df['carts_user'] + df['purchases_user'] + df['favourites_user'] + EPSILON)\n",
    "\n",
    "# ratio of each action type for a given merchant\n",
    "df['clicks_merchant_ratio'] = df['clicks_merchant'] / (df['clicks_merchant'] + df['carts_merchant'] + df['purchases_merchant'] + df['favourites_merchant'] + EPSILON)\n",
    "df['carts_merchant_ratio'] = df['carts_merchant'] / (df['clicks_merchant'] + df['carts_merchant'] + df['purchases_merchant'] + df['favourites_merchant'] + EPSILON)\n",
    "df['purchases_merchant_ratio'] = df['purchases_merchant'] / (df['clicks_merchant'] + df['carts_merchant'] + df['purchases_merchant'] + df['favourites_merchant'] + EPSILON)\n",
    "df['favourites_merchant_ratio'] = df['favourites_merchant'] / (df['clicks_merchant'] + df['carts_merchant'] + df['purchases_merchant'] + df['favourites_merchant'] + EPSILON)\n",
    "\n",
    "# ratio of each action type for a given pair (user, merchant)\n",
    "df['clicks_user_merchant_ratio'] = df['clicks_user_merchant'] / (df['clicks_user_merchant'] + df['carts_user_merchant'] + df['purchases_user_merchant'] + df['favourites_user_merchant'] + EPSILON)\n",
    "df['carts_user_merchant_ratio'] = df['carts_user_merchant'] / (df['clicks_user_merchant'] + df['carts_user_merchant'] + df['purchases_user_merchant'] + df['favourites_user_merchant'] + EPSILON)\n",
    "df['purchases_user_merchant_ratio'] = df['purchases_user_merchant'] / (df['clicks_user_merchant'] + df['carts_user_merchant'] + df['purchases_user_merchant'] + df['favourites_user_merchant'] + EPSILON)\n",
    "df['favourites_user_merchant_ratio'] = df['favourites_user_merchant'] / (df['clicks_user_merchant'] + df['carts_user_merchant'] + df['purchases_user_merchant'] + df['favourites_user_merchant'] + EPSILON)\n",
    "\n",
    "# interval features\n",
    "to_merge = (users['time_stamp'].max() - users['time_stamp'].min()).rename('interval')\n",
    "df = df.merge(to_merge, on='user_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double11 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double11_day = 184\n",
    "# to_merge = (df_user_log[df_user_log['time_stamp'] == double11_day]).reset_index(drop=True)\n",
    "# to_merge = to_merge[to_merge['action_type'] == 2].reset_index(drop=True).groupby('user_id').size().reset_index()\n",
    "# df = df.merge(to_merge, on='user_id', how='left').rename(columns={0: 'double11_purchases'})\n",
    "\n",
    "# df['double11_ratio'] = df['double11_purchases'] / df['purchases_user']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max, Mean, Standard deviation and Median on user-merchant actions (grouping by users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clmns = ['clicks_user_merchant', 'carts_user_merchant', 'purchases_user_merchant', 'favourites_user_merchant']\n",
    "\n",
    "to_merge = df.groupby('user_id')[clmns].max().rename(columns={\n",
    "    'clicks_user_merchant': 'clicks_user_merchant_max',\n",
    "    'carts_user_merchant': 'carts_user_merchant_max',\n",
    "    'purchases_user_merchant': 'purchases_user_merchant_max',\n",
    "    'favourites_user_merchant': 'favourites_user_merchant_max'\n",
    "})\n",
    "df = df.merge(to_merge, on='user_id', how='left')\n",
    "\n",
    "to_merge = df.groupby('user_id')[clmns].mean().rename(columns={\n",
    "    'clicks_user_merchant': 'clicks_user_merchant_mean',\n",
    "    'carts_user_merchant': 'carts_user_merchant_mean',\n",
    "    'purchases_user_merchant': 'purchases_user_merchant_mean',\n",
    "    'favourites_user_merchant': 'favourites_user_merchant_mean'\n",
    "})\n",
    "df = df.merge(to_merge, on='user_id', how='left')\n",
    "\n",
    "to_merge = df.groupby('user_id')[clmns].std().rename(columns={\n",
    "    'clicks_user_merchant': 'clicks_user_merchant_std',\n",
    "    'carts_user_merchant': 'carts_user_merchant_std',\n",
    "    'purchases_user_merchant': 'purchases_user_merchant_std',\n",
    "    'favourites_user_merchant': 'favourites_user_merchant_std'\n",
    "}).fillna(0)\n",
    "df = df.merge(to_merge, on='user_id', how='left')\n",
    "\n",
    "to_merge = df.groupby('user_id')[clmns].median().rename(columns={\n",
    "    'clicks_user_merchant': 'clicks_user_merchant_median',\n",
    "    'carts_user_merchant': 'carts_user_merchant_median',\n",
    "    'purchases_user_merchant': 'purchases_user_merchant_median',\n",
    "    'favourites_user_merchant': 'favourites_user_merchant_median'\n",
    "})\n",
    "df = df.merge(to_merge, on='user_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max, Mean, Standard deviation and Median on user-merchant actions (grouping by merchants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_merge = df.groupby('merchant_id')[clmns].max().rename(columns={\n",
    "    'clicks_user_merchant': 'clicks_merchant_user_max',\n",
    "    'carts_user_merchant': 'carts_merchant_user_max',\n",
    "    'purchases_user_merchant': 'purchases_merchant_user_max',\n",
    "    'favourites_user_merchant': 'favourites_merchant_user_max'\n",
    "})\n",
    "df = df.merge(to_merge, on='merchant_id', how='left')\n",
    "\n",
    "to_merge = df.groupby('merchant_id')[clmns].mean().rename(columns={\n",
    "    'clicks_user_merchant': 'clicks_merchant_user_mean',\n",
    "    'carts_user_merchant': 'carts_merchant_user_mean',\n",
    "    'purchases_user_merchant': 'purchases_merchant_user_mean',\n",
    "    'favourites_user_merchant': 'favourites_merchant_user_mean'\n",
    "})\n",
    "df = df.merge(to_merge, on='merchant_id', how='left')\n",
    "\n",
    "to_merge = df.groupby('merchant_id')[clmns].std().rename(columns={\n",
    "    'clicks_user_merchant': 'clicks_merchant_user_std',\n",
    "    'carts_user_merchant': 'carts_merchant_user_std',\n",
    "    'purchases_user_merchant': 'purchases_merchant_user_std',\n",
    "    'favourites_user_merchant': 'favourites_merchant_user_std'\n",
    "}).fillna(0)\n",
    "df = df.merge(to_merge, on='merchant_id', how='left')\n",
    "\n",
    "to_merge = df.groupby('merchant_id')[clmns].median().rename(columns={\n",
    "    'clicks_user_merchant': 'clicks_merchant_user_median',\n",
    "    'carts_user_merchant': 'carts_merchant_user_median',\n",
    "    'purchases_user_merchant': 'purchases_merchant_user_median',\n",
    "    'favourites_user_merchant': 'favourites_merchant_user_median'\n",
    "})\n",
    "df = df.merge(to_merge, on='merchant_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA features\n",
    "pca_df = df.drop(['kind', 'label'], axis=1)\n",
    "pca = PCA(n_components=PCA_COMPONENTS)\n",
    "pca.fit(pca_df)\n",
    "df = df.join(pd.DataFrame(pca.transform(pca_df), index=pca_df.index).add_prefix('pca_'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finishing Touches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['kind'] == 'train'].drop(['kind'], axis=1)\n",
    "df_test = df[df['kind'] == 'test'].drop(['kind', 'label'], axis=1)\n",
    "X, y = df_train.drop(columns='label'), df_train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch best features\n",
    "The xgboost object will train the model with all features, then the booster object (returned after training) can calculate which features best contribute for most information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=VALID_SET_SIZE, random_state=RANDOM_SEED)\n",
    "\n",
    "dtrain = DMatrix(X_train, label=y_train)\n",
    "dvalid = DMatrix(X_valid, label=y_valid)\n",
    "watchlist = [(dvalid, 'valid')]\n",
    "params = {\n",
    "    'max_depth': 7,\n",
    "    'min_child_weight': 200, \n",
    "    'colsample_bytree': 0.8, \n",
    "    'subsample': 0.8, \n",
    "    'eta': 0.04,    \n",
    "    'seed': RANDOM_SEED,\n",
    "    'eval_metric': 'auc'\n",
    "}\n",
    "booster = train(params, dtrain, num_boost_round=2000, evals=watchlist, early_stopping_rounds=50, verbose_eval=False)\n",
    "best_features = pd.DataFrame(booster.get_score(importance_type='gain').items(), columns=['features', 'importance'])['features'].to_numpy()\n",
    "X = X[best_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO tuning\n",
    "models = {\n",
    "    # 'RandomForestClassifier': [RandomForestClassifier, {\n",
    "    #     'oob_score': True, \n",
    "    #     'n_estimators': 1000, \n",
    "    #     'max_depth': 10, \n",
    "    #     'max_features': 'sqrt',\n",
    "    #     'n_jobs': -1\n",
    "    # }],\n",
    "    'CatBoostClassifier': [CatBoostClassifier, {\n",
    "        'depth': 6,\n",
    "        'learning_rate': 0.05,\n",
    "        'iterations': 1200,\n",
    "        'eval_metric': 'AUC',\n",
    "        'random_state': RANDOM_SEED,\n",
    "        'thread_count': 8,\n",
    "        'silent': True\n",
    "    }],\n",
    "    'LGBMClassifier': [LGBMClassifier, {\n",
    "        'n_estimators': 2000,\n",
    "        'max_depth': 8,\n",
    "        'num_leaves': 50,\n",
    "        'learning_rate': 0.03,\n",
    "        'reg_lambda': 1,\n",
    "        'objective': 'binary',\n",
    "        'metric': ['auc'],\n",
    "        'random_state': RANDOM_SEED,\n",
    "        'n_jobs': -1\n",
    "    }],\n",
    "    'XGBClassifier': [XGBClassifier, {\n",
    "        'max_depth': 7,\n",
    "        'n_estimators': 1000,\n",
    "        'min_child_weight': 200,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'subsample': 0.8,\n",
    "        'eta': 0.04,\n",
    "        'objective': 'binary:logistic',\n",
    "        'use_label_encoder': False,\n",
    "        'seed': RANDOM_SEED\n",
    "    }]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training + Cross Validation (10Fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = {}\n",
    "X, y = X.to_numpy(), y.to_numpy() \n",
    "\n",
    "for name, model_params in models.items():\n",
    "\n",
    "    _class, params = model_params\n",
    "    model_records = {'best_score': 0, 'scores': []}\n",
    "\n",
    "    kf = KFold(n_splits=SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "    for train_index, valid_index in kf.split(X):\n",
    "    \n",
    "        X_train, X_valid = X[train_index], X[valid_index]\n",
    "        y_train, y_valid = y[train_index], y[valid_index]\n",
    "\n",
    "        model = _class(**params)\n",
    "\n",
    "        fit_params = {\n",
    "            'RandomForestClassifier': {}, \n",
    "            'CatBoostClassifier': {},\n",
    "            'LGBMClassifier': {},\n",
    "            'XGBClassifier': {\n",
    "                'eval_metric': 'auc',\n",
    "                'eval_set': [(X_train, y_train), (X_valid, y_valid)],\n",
    "                'early_stopping_rounds': 50,\n",
    "                'verbose': False\n",
    "            }\n",
    "        }\n",
    "\n",
    "        model.fit(X_train, y_train, **fit_params[name])\n",
    "        predictions = model.predict_proba(X_valid)[:,1]\n",
    "        model_records['scores'].append(roc_auc_score(y_valid, predictions))\n",
    "        if model_records['scores'][-1] > model_records['best_score']:\n",
    "            model_records['best_score'] = model_records['scores'][-1]\n",
    "            model_records['best_instance'] = model\n",
    "\n",
    "    records[name] = model_records\n",
    "\n",
    "    print(f'% {name} %')\n",
    "    print('mean score: {0:.4f}'.format(np.mean(model_records['scores'])))\n",
    "    print('best score: {0:.4f}'.format(model_records['best_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Submission**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Predictions from Best Model Instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights calculated from the normalized model scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_instances = [\n",
    "    records['CatBoostClassifier']['best_instance'],\n",
    "    records['LGBMClassifier']['best_instance'],\n",
    "    records['XGBClassifier']['best_instance']\n",
    "]\n",
    "\n",
    "best_scores = [\n",
    "    records['CatBoostClassifier']['best_score'],\n",
    "    records['LGBMClassifier']['best_score'],\n",
    "    records['XGBClassifier']['best_score']\n",
    "]\n",
    "best_scores.append(np.min(best_scores)*0.99)\n",
    "\n",
    "weights = (best_scores-np.min(best_scores))/(np.max(best_scores)-np.min(best_scores))\n",
    "\n",
    "prob_submission = np.zeros(df_test.shape[0])\n",
    "for i in range(len(best_instances)):\n",
    "    prob_submission += best_instances[i].predict_proba(df_test[best_features].to_numpy())[:, 1]*weights[i]\n",
    "prob_submission = prob_submission/np.sum(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights calculated manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'CatBoostClassifier': 0.3,\n",
    "    'LGBMClassifier': 0.1,\n",
    "    'XGBClassifier': 0.6\n",
    "}\n",
    "\n",
    "prob_submission = np.zeros(df_test.shape[0])\n",
    "for name, weight in weights.items():\n",
    "    prob_submission += records[name]['best_instance'].predict_proba(df_test[best_features].to_numpy())[:, 1]*weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print predictions to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = df_test.iloc[:,:2].join(pd.DataFrame(prob_submission, index=df_test.index).rename(columns={0:'prob'}))\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52143a4c1d3c54e78ec3a6dc4e56afb88afff4ca0e9824f0ff9251ac34bb2b12"
  },
  "kernelspec": {
   "display_name": "PyCharm (Project)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}